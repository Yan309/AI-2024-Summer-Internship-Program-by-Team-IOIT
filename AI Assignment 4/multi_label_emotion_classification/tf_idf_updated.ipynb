{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<center> <h3 style=\"color:blue\">========================================================</h3> </center>\n",
    "<center> <h3 style=\"color:green\">In the Name of Allah, the Most Beneficent, the Most Merciful</h3> </center>\n",
    "<center> <h3 style=\"color:blue\">========================================================</h3> </center>\n",
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUTHOR DETAILS and System Details\n",
    "\n",
    "---\n",
    "\n",
    "**Project Title**  \n",
    "*Sentiment Analysis Prediction System*\n",
    "\n",
    "**Author**  \n",
    "*Dr. Rao Adeel Nawab*\n",
    "\n",
    "**Copyright**  \n",
    "&copy; 2024 Dr. Rao Adeel Nawab\n",
    "\n",
    "**License**  \n",
    "*Public Domain*\n",
    "\n",
    "**Version**  \n",
    "*1.0*\n",
    "\n",
    "**Python Version**\n",
    "*3.10.0*\n",
    "\n",
    "**Ssytem Information**\n",
    "*Windows: Edition\tWindows 10 Pro*\n",
    "*Version\t22H2*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<center> <h2 style=\"color:green\">-------------------- PROJECT PURPOSE --------------------</h2> </center>\n",
    "<br>\n",
    "<center><h3>\n",
    "The main purpose of this project is to demonstrate how the Sentiment Analysis problem can be treated as a Supervised Machine Learning problem using Python and the Scikit-learn toolkit.\n",
    "</h3></center>\n",
    "<br>\n",
    "<center><h3>For this purpose, In Sha Allah, we will execute the Machine Learning cycle.</h3></center>\n",
    "<br>\n",
    "<center> <h2 style=\"color:green\">-------------------------------------------------------------------------</h2> </center>\n",
    "<br><br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red\">Sentiment Analysis Prediction System ‚Äì Machine Learning Cycle</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Cycle\n",
    "\n",
    "### Four phases of a Machine Learning Cycle are\n",
    "\n",
    "### Training Phase\n",
    "\n",
    "    Build the Model using Training Data\n",
    "\n",
    "### Testing Phase\n",
    "\n",
    "     Evaluate the performance of Model using Testing Data\n",
    "\n",
    "### Application Phase\n",
    "\n",
    "     Deploy the Model in the Real-world, to predict Real-time unseen Data\n",
    "\n",
    "### Feedback Phase\n",
    "\n",
    "    Take Feedback from the Users and Domain Experts to improve the Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Sha Allah, we will follow the following steps to execute the Machine Learning Cycle Using a Single File\n",
    "\n",
    "#### Step 1: Import Libraries\n",
    "\n",
    "#### Step 2: Load Sample Data\n",
    "\n",
    "    Step 2.1: View Columns In Dataset\n",
    "    Step 2.2: Keeping Required Columns In Dataset\n",
    "    \n",
    "\n",
    "#### Step 3: Understand and Pre-process Sample Data\n",
    "    \n",
    "    Step 3.1: Download and set stopwords\n",
    "\n",
    "    todo tell about task and also if not remove stop words what will be the effects\n",
    "\n",
    "    run with out cleaning, run with cleaning and dont remove stop words.\n",
    "\n",
    "    Step 3.2: Define a function to clean the text\n",
    "\n",
    "    This function will remove symbols and numbers, convert text to lowercase, and remove stop words.\n",
    "\n",
    "    Step 3.3: Load the data\n",
    "\n",
    "    Step 3.4: Drop rows with NaN values in the text column\n",
    "\n",
    "    Step 3.5: Apply Data Cleaning\n",
    "\n",
    "    Step 3.6: Data After Processing\n",
    "\n",
    "    Step 3.7: Saving Cleaned Data as Seperate CSV File\n",
    "\n",
    "\n",
    "#### Step 4: Splitting Sample Data into Training Data and Testing Data \n",
    "\n",
    "#### Step 5: Label Encoding (Input and Output is converted in Numeric Representation)\n",
    "\n",
    "    Output is already in Numeric so we not need the Label Encoding.  \n",
    "\n",
    "#### Step 6: Execute the Training Phase\n",
    "\n",
    "\n",
    "    Step 6.1: Training Data and Testing Data\n",
    "\n",
    "    Step 6.2: Train the Model\n",
    "\n",
    "    Step 6.3: Save the Trained Model\n",
    "\n",
    "#### Step 7: Execute the Testing Phase \n",
    "\n",
    "    Step 7.1: Load the Saved Model\n",
    "    \n",
    "    Step 7.2: Evluate the Machine Learning Model\n",
    "\n",
    "    Step 7.3: Showing Confusion Matrix\n",
    "\n",
    "\n",
    "#### Step 8: Execute the Application Phase \n",
    "\n",
    "    Step 8.1: Take Input from User, Preprocess it\n",
    "\n",
    "    Step 8.4: Load the Saved Model\n",
    "\n",
    "    Step 8.5: Model Prediction\n",
    "\n",
    "         Step 8.5.1: Apply Model on the Label Encoded Feature Vector of unseen instance and return Prediction to the User\n",
    "\n",
    "#### Step 9: Execute the Feedback Phase \n",
    "\n",
    "#### Step 10: Improve the Model based on Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import nltk\n",
    "import joblib\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Load Sample Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Original data can be download from\n",
    "http://saifmohammad.com/WebDocs/AIT-2018/AIT2018-DATA/SemEval2018-Task1-all-data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Sentiment Analysis Data:\n",
      "============\n",
      "\n",
      "Sample data count = 110\n",
      "\n",
      "              ID                                              Tweet  anger  \\\n",
      "0  2017-En-10331                        Need a new outlet for #rage      1   \n",
      "1  2017-En-21898  Arguing with these people doesn't work anyway,...      1   \n",
      "2  2017-En-10398  When your sister is 19 and throws legitimate t...      1   \n",
      "3  2017-En-40975  #HRmanagement must discourage the expediency f...      1   \n",
      "4  2017-En-30811  It is too fucking bright &amp; too fucking hot...      1   \n",
      "\n",
      "   anticipation  disgust  fear  joy  love  optimism  pessimism  sadness  \\\n",
      "0             0        1     0    0     0         0          0        0   \n",
      "1             0        1     1    0     0         0          0        0   \n",
      "2             0        1     0    0     0         0          0        0   \n",
      "3             0        1     0    0     0         1          0        0   \n",
      "4             0        1     0    0     0         0          0        0   \n",
      "\n",
      "   surprise  trust  \n",
      "0         0      0  \n",
      "1         0      0  \n",
      "2         0      0  \n",
      "3         0      0  \n",
      "4         0      0  \n",
      "                ID                                              Tweet  anger  \\\n",
      "105  2017-En-21181  @AlaskaGurus @adventuretweets agreed! üòç an awe...      0   \n",
      "106  2017-En-21742  Don't be #afraid of the space between your #dr...      0   \n",
      "107  2017-En-30853  @GameGrumps THANK YOU SO MUCH FOR COMING TO DE...      0   \n",
      "108  2017-En-30150  And by the way, Takeru's eyes sparkling while ...      0   \n",
      "109  2017-En-30283  Accept the challenges so that you can feel the...      0   \n",
      "\n",
      "     anticipation  disgust  fear  joy  love  optimism  pessimism  sadness  \\\n",
      "105             0        0     0    1     0         0          0        0   \n",
      "106             0        0     1    0     0         1          0        0   \n",
      "107             0        0     0    1     1         1          0        0   \n",
      "108             0        0     0    1     1         1          0        0   \n",
      "109             0        0     0    1     0         1          0        0   \n",
      "\n",
      "     surprise  trust  \n",
      "105         0      1  \n",
      "106         0      1  \n",
      "107         0      1  \n",
      "108         0      1  \n",
      "109         0      1  \n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('sampled_emotion_data.csv')\n",
    "\n",
    "print(\"\\n\\nSentiment Analysis Data:\")\n",
    "print(\"============\\n\")\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "print(f'Sample data count = {len(data)}\\n')\n",
    "print(data.head())\n",
    "print(data.tail())\n",
    "# sentiment_type\n",
    "# 1 is positive\n",
    "# 0 is positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.1: View Columns In Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'Tweet', 'anger', 'anticipation', 'disgust', 'fear', 'joy',\n",
       "       'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Understand and Pre-process Sample Data\n",
    "\n",
    "## Definition of Pre-processing Sample Data\n",
    "\n",
    "Pre-processing sample data involves cleaning and transforming raw data into a format that can be effectively used for analysis. This step is crucial in natural language processing (NLP) as it helps in improving the performance of machine learning models by removing noise and ensuring consistency.\n",
    "\n",
    "## Impact of Pre-processing Sample Data\n",
    "\n",
    "1. **Improves Data Quality**: Removes irrelevant and redundant information, leading to cleaner and more meaningful data.\n",
    "2. **Enhances Model Accuracy**: By reducing noise and standardizing text, pre-processing helps in achieving better model performance.\n",
    "3. **Facilitates Efficient Data Analysis**: Simplifies the data, making it easier to analyze and interpret.\n",
    "4. **Reduces Complexity**: Helps in reducing the complexity of data by normalizing text and handling missing values.\n",
    "\n",
    "Common pre-processing steps include:\n",
    "- Removing punctuation and special characters\n",
    "- Converting text to lowercase\n",
    "- Removing stopwords\n",
    "- Stemming and lemmatization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.1: Download and set stopwords\n",
    "\n",
    "### Definition of Stopwords\n",
    "\n",
    "Stopwords are commonly used words in a language (such as \"the\", \"is\", \"in\", etc.) that are often filtered out before processing text data. These words are considered to have little value in understanding the content of a document because they are so frequently used.\n",
    "\n",
    "### Impact of Removing Stopwords\n",
    "\n",
    "1. **Reduces Noise**: Eliminates common but unimportant words, helping to focus on the more meaningful words in the text.\n",
    "2. **Improves Model Performance**: Reduces the dimensionality of the data, which can improve the efficiency and accuracy of machine learning models.\n",
    "3. **Enhances Text Analysis**: Helps in highlighting the significant words that contribute to the context and meaning of the text.\n",
    "\n",
    "### Note\n",
    "\n",
    "It is important to understand that removing stopwords is not always necessary and depends on the specific requirements of your text analysis or machine learning task. In some cases, stopwords might carry important contextual information that could be valuable for your analysis. Therefore, it is essential to evaluate whether removing stopwords will benefit or hinder your particular application.\n",
    "\n",
    "### How to Download and Set Stopwords\n",
    "\n",
    "To use stopwords in your text pre-processing steps, you need to download a list of stopwords for the language you are working with. In the case of English, the `nltk` library provides a comprehensive list of stopwords.\n",
    "\n",
    "The following code snippet shows how to download and set stopwords using the `nltk` library:\n",
    "\n",
    "```python\n",
    "# Ensure you have downloaded the stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Set the stopwords for English\n",
    "stop_words = set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure you have downloaded the stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.2: Define a function to clean the text\n",
    "\n",
    "This function will remove symbols and numbers, convert text to lowercase, and remove stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean the text\n",
    "def clean_text(text):\n",
    "    # Remove symbols and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove stop words\n",
    "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the sake of clear understanding we will divide clean_text function in seperate function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function 1: Remove Symbols and Numbers\n",
    "\n",
    "This function removes all symbols and numbers from the text, leaving only alphabetic characters. This step is important to ensure that the text data is clean and only contains meaningful words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_symbols_numbers(text):\n",
    "    # Remove symbols and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function 2: Convert Text to Lowercase\n",
    "This function converts all characters in the text to lowercase to ensure consistency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lowercase(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function 3: Remove Stopwords\n",
    "This function removes stopwords from the text, which are common words that may not contribute significant meaning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    # Remove stop words\n",
    "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.3: Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv('sampled_emotion_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>love</th>\n",
       "      <th>optimism</th>\n",
       "      <th>pessimism</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-En-10331</td>\n",
       "      <td>Need a new outlet for #rage</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-En-21898</td>\n",
       "      <td>Arguing with these people doesn't work anyway,...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-En-10398</td>\n",
       "      <td>When your sister is 19 and throws legitimate t...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-En-40975</td>\n",
       "      <td>#HRmanagement must discourage the expediency f...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-En-30811</td>\n",
       "      <td>It is too fucking bright &amp;amp; too fucking hot...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2017-En-10004</td>\n",
       "      <td>Don't join @BTCare they put the phone down on ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2017-En-11599</td>\n",
       "      <td>Don't be offended,\\nI'm just doing something t...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2017-En-11573</td>\n",
       "      <td>#Azerbaijan #Baku Azerbaijan to prevent anothe...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2017-En-11250</td>\n",
       "      <td>and i will strike down upon thee with great ve...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2017-En-21706</td>\n",
       "      <td>Hey @PapaJohnsUK I've been charged ¬£40.24 on m...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2017-En-40612</td>\n",
       "      <td>No quite sure how Craig Gordon's stayed on the...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2017-En-10642</td>\n",
       "      <td>one month til someone's bday and i think it's ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2017-En-21541</td>\n",
       "      <td>Shriekfest is lining up VOLUNTEERS! Oct 6-9, o...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2017-En-11635</td>\n",
       "      <td>This makes things easier and compact and less ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2017-En-10738</td>\n",
       "      <td>@StutteringGiant at least his character in fas...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2017-En-40668</td>\n",
       "      <td>@rickygervais my first time in Slough so check...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2017-En-30990</td>\n",
       "      <td>@ruthwalford95 you may be right, but since yea...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2017-En-40629</td>\n",
       "      <td>New job training= too much meat eating! Oh wel...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2017-En-20207</td>\n",
       "      <td>@ANI_news #Pakistan should stop cross border #...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2017-En-40218</td>\n",
       "      <td>When you have 15 doe run the opposite side of ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2017-En-30556</td>\n",
       "      <td>Riggs dumb ass hell lolol  #LethalWeapon</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2017-En-30880</td>\n",
       "      <td>Metal keeps you young and spry and keeps your ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2017-En-20589</td>\n",
       "      <td>Farting in a hot are car when your windows don...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2017-En-11146</td>\n",
       "      <td>Pro Tip: Go back to work when your kid reaches...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2017-En-40336</td>\n",
       "      <td>Feels grim not having your nails done</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2017-En-20758</td>\n",
       "      <td>@Meruna_  as a musician, I can tell you that m...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2017-En-10527</td>\n",
       "      <td>@thomeagle Just to help maintain and boost our...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2017-En-41079</td>\n",
       "      <td>I wonder if the #wolfcreek TV show is sponsore...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2017-En-11066</td>\n",
       "      <td>I don't like pineapple I only eat them on pizz...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2017-En-11311</td>\n",
       "      <td>@Bell @Bell_Support Cancelling home Fibe, Inte...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2017-En-22088</td>\n",
       "      <td>I'm a walking ball of stress and anxiety lol</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2017-En-11018</td>\n",
       "      <td>I just killed a spider so big it sprayed spide...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2017-En-21681</td>\n",
       "      <td>#heavyheart these last couple of days, who are...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2017-En-22065</td>\n",
       "      <td>The anxiety I have right nowüò≠üò≠üò≠</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2017-En-20296</td>\n",
       "      <td>@GuardianAus @Paul_Karp Maybe 49% support no #...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2017-En-21867</td>\n",
       "      <td>@Mista_Nightmare i dont understand why u do vi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2017-En-21765</td>\n",
       "      <td>@wittyneeraj What is Shehla Rashid &amp;amp; Kaniy...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2017-En-40226</td>\n",
       "      <td>I have no clue where my charger is... #lost</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2017-En-30799</td>\n",
       "      <td>@LaurenBrierley2 sparkling water = death</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2017-En-20409</td>\n",
       "      <td>@jade0208 unfortunately the diet is still on, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2017-En-30255</td>\n",
       "      <td>I'm so playful. lol I need somebody that'll jo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2017-En-22184</td>\n",
       "      <td>The Pats are awesome. Belichick is awesome ......</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2017-En-22019</td>\n",
       "      <td>Beginning the process to see if working is an ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2017-En-40552</td>\n",
       "      <td>I'm onto you, @Sargon_of_Akkad, I know you sec...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2017-En-11332</td>\n",
       "      <td>@RadioX @ChrisMoyles wow. not heard this in fo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2017-En-41428</td>\n",
       "      <td>lost 11 lbs since I got married (eating health...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2017-En-10844</td>\n",
       "      <td>@Aurena1701 @AngryOrchard he didn't sting me l...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2017-En-30640</td>\n",
       "      <td>backed pats -2.5 10/11 just before #pleasing</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2017-En-30645</td>\n",
       "      <td>GKN so lively as well, mad quick</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2017-En-30013</td>\n",
       "      <td>My morning started off amazing!! Hopefully the...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2017-En-41179</td>\n",
       "      <td>@phil500 \\nSo sadden \\nSpunky a beautiful dog\\...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2017-En-10652</td>\n",
       "      <td>also other ppl who i love a whole damn lot; er...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>2017-En-30028</td>\n",
       "      <td>@Singaholic121 Good morning, love! Happy first...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>2017-En-30907</td>\n",
       "      <td>#GBBO can cheer me up ‚ò∫Ô∏è</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>2017-En-21138</td>\n",
       "      <td>i've been rooting for him since the beginning ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>2017-En-30954</td>\n",
       "      <td>I just want to let everyone out there know you...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>2017-En-21412</td>\n",
       "      <td>this girl at my new school is so pretty i am</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>2017-En-22081</td>\n",
       "      <td>@mcrichard awe that's adorable</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>2017-En-30757</td>\n",
       "      <td>noah fence but i want a harley quinn or blake ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>2017-En-30579</td>\n",
       "      <td>@Devilligan It's a beautifully sincere balanci...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>2017-En-31340</td>\n",
       "      <td>@jtkola @danmericaCNN she's still younger and ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>2017-En-21132</td>\n",
       "      <td>When you make a great tasting shake and no wor...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>2017-En-10917</td>\n",
       "      <td>I think our defense here at USC is playing wel...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>2017-En-30234</td>\n",
       "      <td>In ever use to like smiling until I realized h...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>2017-En-21938</td>\n",
       "      <td>#2 complained then while his head and then cal...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>2017-En-21906</td>\n",
       "      <td>When @idinamenzel says she's releasing more #I...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>2017-En-20398</td>\n",
       "      <td>While we focus on issue of #IPCA @IHFOKids Ind...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>2017-En-30394</td>\n",
       "      <td>@AimiSyafiqahR go check up with your bf. He'll...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>2017-En-30026</td>\n",
       "      <td>@WSJNordics You make the world a more joyful p...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>2017-En-21276</td>\n",
       "      <td>The ecosystem is meant to break thru the wall ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>2017-En-31265</td>\n",
       "      <td>After this news I'm supposed to be so damn hap...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>2017-En-41159</td>\n",
       "      <td>Not written for African-American\\n\\nNo refuge ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>2017-En-10067</td>\n",
       "      <td>Once again the only thing on my feed is naay r...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>2017-En-10485</td>\n",
       "      <td>And she got all angry telling me 'but what wou...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>2017-En-30711</td>\n",
       "      <td>god, Facebook's design has started to remind m...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>2017-En-20426</td>\n",
       "      <td>@myaeggs I can't get a better look at her bc I...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>2017-En-20788</td>\n",
       "      <td>@RoWillFindYou Eric couldn't help but laugh, t...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>2017-En-10391</td>\n",
       "      <td>I miss doing nothing someone I care about and ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>2017-En-41232</td>\n",
       "      <td>feel really sad and down todayüòí</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>2017-En-20785</td>\n",
       "      <td>Not sure that men can handle a woman that's go...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>2017-En-30479</td>\n",
       "      <td>When we give cheerfully and accept gratefully,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>2017-En-10888</td>\n",
       "      <td>The rage has died down.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>2017-En-10979</td>\n",
       "      <td>@XboxMAD @RobotBrush Ballmer will be furious. ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>2017-En-10457</td>\n",
       "      <td>@simon_penn_r @AntisocialJW2 has turned into a...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>2017-En-40755</td>\n",
       "      <td>@hannah_2401 hannah stop being mournful and ch...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>2017-En-40397</td>\n",
       "      <td>@RaveenElexiis I fought a racist girl and she ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>2017-En-40371</td>\n",
       "      <td>Don't wanna go to work but I want the money</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>2017-En-10233</td>\n",
       "      <td>@luxbet Did you even give out any pizzas ? Ser...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>2017-En-11302</td>\n",
       "      <td>@jbanks88 offense can't score 3 redzone trips ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>2017-En-40141</td>\n",
       "      <td>And I won't even get started with Hillary and ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>2017-En-20474</td>\n",
       "      <td>You don't know what to expect by Brendon's vid...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>2017-En-40667</td>\n",
       "      <td>Are you serious??</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>2017-En-22178</td>\n",
       "      <td>@samsteinhp stop the presses, @realDonaldTrump...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>2017-En-30310</td>\n",
       "      <td>@NHLstoreNYC I'm cheering for @TeamNA_WCH and ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>2017-En-10385</td>\n",
       "      <td>@Trump_Videos she looks completely #rabid @rea...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>2017-En-20439</td>\n",
       "      <td>@vibaby @imEB y'all had crackheads in the isla...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>2017-En-20761</td>\n",
       "      <td>@Policy_Exchange A plus point, she won't have ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>2017-En-21977</td>\n",
       "      <td>I'm in awe</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>2017-En-10838</td>\n",
       "      <td>Vale! Vale! Sip sangria and taste tantalizing ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>2017-En-21397</td>\n",
       "      <td>#tulsa - Police manufacture murder... Wonder w...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>2017-En-11323</td>\n",
       "      <td>@RVAGameBreak @GAHSBasketball @GAJagsFootball ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>2017-En-21783</td>\n",
       "      <td>üòÇ - youre one of those whod bully me omg but i...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>2017-En-30149</td>\n",
       "      <td>#ukedchat A4 Just go outside (or to the gym ha...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>2017-En-30140</td>\n",
       "      <td>Zephaniah 3:17 He surely is rejoicing over us ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>2017-En-40870</td>\n",
       "      <td>Refuse to let myself get discouraged.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>2017-En-21181</td>\n",
       "      <td>@AlaskaGurus @adventuretweets agreed! üòç an awe...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>2017-En-21742</td>\n",
       "      <td>Don't be #afraid of the space between your #dr...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>2017-En-30853</td>\n",
       "      <td>@GameGrumps THANK YOU SO MUCH FOR COMING TO DE...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>2017-En-30150</td>\n",
       "      <td>And by the way, Takeru's eyes sparkling while ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>2017-En-30283</td>\n",
       "      <td>Accept the challenges so that you can feel the...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                ID                                              Tweet  anger  \\\n",
       "0    2017-En-10331                        Need a new outlet for #rage      1   \n",
       "1    2017-En-21898  Arguing with these people doesn't work anyway,...      1   \n",
       "2    2017-En-10398  When your sister is 19 and throws legitimate t...      1   \n",
       "3    2017-En-40975  #HRmanagement must discourage the expediency f...      1   \n",
       "4    2017-En-30811  It is too fucking bright &amp; too fucking hot...      1   \n",
       "5    2017-En-10004  Don't join @BTCare they put the phone down on ...      1   \n",
       "6    2017-En-11599  Don't be offended,\\nI'm just doing something t...      1   \n",
       "7    2017-En-11573  #Azerbaijan #Baku Azerbaijan to prevent anothe...      1   \n",
       "8    2017-En-11250  and i will strike down upon thee with great ve...      1   \n",
       "9    2017-En-21706  Hey @PapaJohnsUK I've been charged ¬£40.24 on m...      1   \n",
       "10   2017-En-40612  No quite sure how Craig Gordon's stayed on the...      0   \n",
       "11   2017-En-10642  one month til someone's bday and i think it's ...      1   \n",
       "12   2017-En-21541  Shriekfest is lining up VOLUNTEERS! Oct 6-9, o...      0   \n",
       "13   2017-En-11635  This makes things easier and compact and less ...      0   \n",
       "14   2017-En-10738  @StutteringGiant at least his character in fas...      0   \n",
       "15   2017-En-40668  @rickygervais my first time in Slough so check...      0   \n",
       "16   2017-En-30990  @ruthwalford95 you may be right, but since yea...      0   \n",
       "17   2017-En-40629  New job training= too much meat eating! Oh wel...      0   \n",
       "18   2017-En-20207  @ANI_news #Pakistan should stop cross border #...      1   \n",
       "19   2017-En-40218  When you have 15 doe run the opposite side of ...      0   \n",
       "20   2017-En-30556           Riggs dumb ass hell lolol  #LethalWeapon      1   \n",
       "21   2017-En-30880  Metal keeps you young and spry and keeps your ...      1   \n",
       "22   2017-En-20589  Farting in a hot are car when your windows don...      1   \n",
       "23   2017-En-11146  Pro Tip: Go back to work when your kid reaches...      1   \n",
       "24   2017-En-40336              Feels grim not having your nails done      0   \n",
       "25   2017-En-20758  @Meruna_  as a musician, I can tell you that m...      1   \n",
       "26   2017-En-10527  @thomeagle Just to help maintain and boost our...      1   \n",
       "27   2017-En-41079  I wonder if the #wolfcreek TV show is sponsore...      1   \n",
       "28   2017-En-11066  I don't like pineapple I only eat them on pizz...      0   \n",
       "29   2017-En-11311  @Bell @Bell_Support Cancelling home Fibe, Inte...      1   \n",
       "30   2017-En-22088       I'm a walking ball of stress and anxiety lol      0   \n",
       "31   2017-En-11018  I just killed a spider so big it sprayed spide...      1   \n",
       "32   2017-En-21681  #heavyheart these last couple of days, who are...      0   \n",
       "33   2017-En-22065                    The anxiety I have right nowüò≠üò≠üò≠      0   \n",
       "34   2017-En-20296  @GuardianAus @Paul_Karp Maybe 49% support no #...      0   \n",
       "35   2017-En-21867  @Mista_Nightmare i dont understand why u do vi...      1   \n",
       "36   2017-En-21765  @wittyneeraj What is Shehla Rashid &amp; Kaniy...      0   \n",
       "37   2017-En-40226        I have no clue where my charger is... #lost      0   \n",
       "38   2017-En-30799           @LaurenBrierley2 sparkling water = death      0   \n",
       "39   2017-En-20409  @jade0208 unfortunately the diet is still on, ...      0   \n",
       "40   2017-En-30255  I'm so playful. lol I need somebody that'll jo...      0   \n",
       "41   2017-En-22184  The Pats are awesome. Belichick is awesome ......      0   \n",
       "42   2017-En-22019  Beginning the process to see if working is an ...      0   \n",
       "43   2017-En-40552  I'm onto you, @Sargon_of_Akkad, I know you sec...      0   \n",
       "44   2017-En-11332  @RadioX @ChrisMoyles wow. not heard this in fo...      0   \n",
       "45   2017-En-41428  lost 11 lbs since I got married (eating health...      0   \n",
       "46   2017-En-10844  @Aurena1701 @AngryOrchard he didn't sting me l...      0   \n",
       "47   2017-En-30640       backed pats -2.5 10/11 just before #pleasing      0   \n",
       "48   2017-En-30645                   GKN so lively as well, mad quick      0   \n",
       "49   2017-En-30013  My morning started off amazing!! Hopefully the...      0   \n",
       "50   2017-En-41179  @phil500 \\nSo sadden \\nSpunky a beautiful dog\\...      0   \n",
       "51   2017-En-10652  also other ppl who i love a whole damn lot; er...      0   \n",
       "52   2017-En-30028  @Singaholic121 Good morning, love! Happy first...      0   \n",
       "53   2017-En-30907                           #GBBO can cheer me up ‚ò∫Ô∏è      0   \n",
       "54   2017-En-21138  i've been rooting for him since the beginning ...      0   \n",
       "55   2017-En-30954  I just want to let everyone out there know you...      0   \n",
       "56   2017-En-21412      this girl at my new school is so pretty i am       0   \n",
       "57   2017-En-22081                     @mcrichard awe that's adorable      0   \n",
       "58   2017-En-30757  noah fence but i want a harley quinn or blake ...      0   \n",
       "59   2017-En-30579  @Devilligan It's a beautifully sincere balanci...      0   \n",
       "60   2017-En-31340  @jtkola @danmericaCNN she's still younger and ...      0   \n",
       "61   2017-En-21132  When you make a great tasting shake and no wor...      0   \n",
       "62   2017-En-10917  I think our defense here at USC is playing wel...      0   \n",
       "63   2017-En-30234  In ever use to like smiling until I realized h...      0   \n",
       "64   2017-En-21938  #2 complained then while his head and then cal...      0   \n",
       "65   2017-En-21906  When @idinamenzel says she's releasing more #I...      0   \n",
       "66   2017-En-20398  While we focus on issue of #IPCA @IHFOKids Ind...      0   \n",
       "67   2017-En-30394  @AimiSyafiqahR go check up with your bf. He'll...      0   \n",
       "68   2017-En-30026  @WSJNordics You make the world a more joyful p...      0   \n",
       "69   2017-En-21276  The ecosystem is meant to break thru the wall ...      0   \n",
       "70   2017-En-31265  After this news I'm supposed to be so damn hap...      0   \n",
       "71   2017-En-41159  Not written for African-American\\n\\nNo refuge ...      0   \n",
       "72   2017-En-10067  Once again the only thing on my feed is naay r...      1   \n",
       "73   2017-En-10485  And she got all angry telling me 'but what wou...      1   \n",
       "74   2017-En-30711  god, Facebook's design has started to remind m...      0   \n",
       "75   2017-En-20426  @myaeggs I can't get a better look at her bc I...      0   \n",
       "76   2017-En-20788  @RoWillFindYou Eric couldn't help but laugh, t...      0   \n",
       "77   2017-En-10391  I miss doing nothing someone I care about and ...      0   \n",
       "78   2017-En-41232                    feel really sad and down todayüòí      0   \n",
       "79   2017-En-20785  Not sure that men can handle a woman that's go...      1   \n",
       "80   2017-En-30479  When we give cheerfully and accept gratefully,...      0   \n",
       "81   2017-En-10888                            The rage has died down.      0   \n",
       "82   2017-En-10979  @XboxMAD @RobotBrush Ballmer will be furious. ...      1   \n",
       "83   2017-En-10457  @simon_penn_r @AntisocialJW2 has turned into a...      1   \n",
       "84   2017-En-40755  @hannah_2401 hannah stop being mournful and ch...      0   \n",
       "85   2017-En-40397  @RaveenElexiis I fought a racist girl and she ...      1   \n",
       "86   2017-En-40371       Don't wanna go to work but I want the money       0   \n",
       "87   2017-En-10233  @luxbet Did you even give out any pizzas ? Ser...      1   \n",
       "88   2017-En-11302  @jbanks88 offense can't score 3 redzone trips ...      0   \n",
       "89   2017-En-40141  And I won't even get started with Hillary and ...      0   \n",
       "90   2017-En-20474  You don't know what to expect by Brendon's vid...      1   \n",
       "91   2017-En-40667                                  Are you serious??      0   \n",
       "92   2017-En-22178  @samsteinhp stop the presses, @realDonaldTrump...      1   \n",
       "93   2017-En-30310  @NHLstoreNYC I'm cheering for @TeamNA_WCH and ...      0   \n",
       "94   2017-En-10385  @Trump_Videos she looks completely #rabid @rea...      0   \n",
       "95   2017-En-20439  @vibaby @imEB y'all had crackheads in the isla...      0   \n",
       "96   2017-En-20761  @Policy_Exchange A plus point, she won't have ...      1   \n",
       "97   2017-En-21977                                         I'm in awe      0   \n",
       "98   2017-En-10838  Vale! Vale! Sip sangria and taste tantalizing ...      0   \n",
       "99   2017-En-21397  #tulsa - Police manufacture murder... Wonder w...      0   \n",
       "100  2017-En-11323  @RVAGameBreak @GAHSBasketball @GAJagsFootball ...      0   \n",
       "101  2017-En-21783  üòÇ - youre one of those whod bully me omg but i...      0   \n",
       "102  2017-En-30149  #ukedchat A4 Just go outside (or to the gym ha...      0   \n",
       "103  2017-En-30140  Zephaniah 3:17 He surely is rejoicing over us ...      0   \n",
       "104  2017-En-40870              Refuse to let myself get discouraged.      0   \n",
       "105  2017-En-21181  @AlaskaGurus @adventuretweets agreed! üòç an awe...      0   \n",
       "106  2017-En-21742  Don't be #afraid of the space between your #dr...      0   \n",
       "107  2017-En-30853  @GameGrumps THANK YOU SO MUCH FOR COMING TO DE...      0   \n",
       "108  2017-En-30150  And by the way, Takeru's eyes sparkling while ...      0   \n",
       "109  2017-En-30283  Accept the challenges so that you can feel the...      0   \n",
       "\n",
       "     anticipation  disgust  fear  joy  love  optimism  pessimism  sadness  \\\n",
       "0               0        1     0    0     0         0          0        0   \n",
       "1               0        1     1    0     0         0          0        0   \n",
       "2               0        1     0    0     0         0          0        0   \n",
       "3               0        1     0    0     0         1          0        0   \n",
       "4               0        1     0    0     0         0          0        0   \n",
       "5               0        1     0    0     0         0          0        0   \n",
       "6               0        1     0    0     0         0          0        1   \n",
       "7               0        0     0    0     0         0          0        0   \n",
       "8               0        1     0    0     0         0          0        0   \n",
       "9               0        1     0    0     0         0          0        1   \n",
       "10              1        0     0    0     0         0          0        0   \n",
       "11              1        0     0    1     0         0          0        0   \n",
       "12              1        0     1    0     0         0          0        0   \n",
       "13              1        0     0    1     0         1          0        0   \n",
       "14              1        0     0    0     0         1          0        0   \n",
       "15              1        0     0    1     0         0          0        0   \n",
       "16              1        0     0    0     0         1          0        0   \n",
       "17              1        0     0    1     0         1          0        0   \n",
       "18              1        0     1    0     0         1          0        0   \n",
       "19              1        0     0    0     0         0          0        1   \n",
       "20              0        1     0    1     0         0          0        0   \n",
       "21              0        1     0    1     0         1          0        0   \n",
       "22              0        1     0    0     0         0          0        1   \n",
       "23              0        1     0    0     0         0          0        1   \n",
       "24              0        1     0    0     0         0          0        1   \n",
       "25              0        1     0    0     0         0          0        0   \n",
       "26              0        1     0    0     0         0          0        1   \n",
       "27              0        1     0    0     0         0          0        0   \n",
       "28              0        1     0    0     0         0          0        0   \n",
       "29              0        1     0    0     0         0          0        1   \n",
       "30              0        0     1    0     0         0          0        0   \n",
       "31              1        1     1    0     0         0          1        0   \n",
       "32              0        0     1    0     0         0          0        1   \n",
       "33              0        0     1    0     0         0          1        1   \n",
       "34              0        0     1    0     0         0          0        0   \n",
       "35              0        1     1    0     0         0          0        0   \n",
       "36              0        1     1    0     0         0          0        0   \n",
       "37              0        0     1    0     0         0          0        1   \n",
       "38              0        0     1    0     0         0          1        0   \n",
       "39              0        0     1    0     0         0          0        1   \n",
       "40              0        0     0    1     1         1          0        0   \n",
       "41              0        0     0    1     1         0          0        0   \n",
       "42              0        0     0    1     0         0          0        1   \n",
       "43              1        0     0    1     0         0          0        0   \n",
       "44              0        0     0    1     0         0          0        0   \n",
       "45              0        0     0    1     0         1          0        0   \n",
       "46              0        0     0    1     0         1          0        0   \n",
       "47              1        0     0    1     0         1          0        0   \n",
       "48              0        0     0    1     0         1          0        0   \n",
       "49              0        0     0    1     1         1          0        0   \n",
       "50              0        0     0    1     1         1          0        1   \n",
       "51              0        0     0    1     1         1          0        0   \n",
       "52              0        0     0    1     1         1          0        0   \n",
       "53              0        0     0    1     1         1          0        0   \n",
       "54              1        0     0    0     1         0          0        0   \n",
       "55              0        0     0    1     1         1          0        0   \n",
       "56              1        0     0    1     1         1          0        0   \n",
       "57              0        0     0    1     1         1          0        0   \n",
       "58              0        0     0    1     1         1          0        0   \n",
       "59              0        0     0    1     1         1          0        0   \n",
       "60              1        0     0    1     1         1          0        0   \n",
       "61              0        0     0    1     0         1          0        0   \n",
       "62              1        0     0    1     0         1          0        0   \n",
       "63              0        0     0    1     0         1          0        0   \n",
       "64              0        0     0    0     0         1          0        0   \n",
       "65              1        0     0    1     0         1          0        0   \n",
       "66              0        0     1    0     0         1          0        0   \n",
       "67              0        0     0    1     1         1          0        0   \n",
       "68              0        0     0    1     1         1          0        0   \n",
       "69              1        0     1    0     0         1          0        0   \n",
       "70              0        1     0    1     0         0          1        1   \n",
       "71              0        0     0    0     0         0          1        1   \n",
       "72              0        1     0    0     0         0          1        0   \n",
       "73              0        1     0    0     0         0          1        0   \n",
       "74              0        1     0    0     0         0          1        0   \n",
       "75              0        0     1    0     0         0          1        1   \n",
       "76              0        1     0    0     0         0          1        1   \n",
       "77              0        0     0    0     0         0          1        1   \n",
       "78              0        0     0    0     0         0          1        1   \n",
       "79              0        1     0    0     0         0          1        1   \n",
       "80              0        0     0    1     0         1          0        1   \n",
       "81              0        0     0    1     0         0          0        1   \n",
       "82              0        1     0    0     0         0          0        1   \n",
       "83              0        0     0    0     0         0          0        1   \n",
       "84              0        1     0    0     0         0          0        1   \n",
       "85              0        1     0    0     0         0          0        1   \n",
       "86              0        1     0    1     0         0          1        1   \n",
       "87              0        1     0    0     0         0          0        1   \n",
       "88              0        1     0    0     0         0          0        1   \n",
       "89              0        1     0    0     0         0          0        1   \n",
       "90              0        1     1    0     0         0          0        0   \n",
       "91              0        1     0    0     0         0          0        0   \n",
       "92              0        1     0    0     0         0          0        0   \n",
       "93              1        0     0    1     0         1          0        0   \n",
       "94              0        0     0    1     0         0          0        0   \n",
       "95              1        1     0    1     0         1          0        0   \n",
       "96              0        1     0    0     0         0          0        0   \n",
       "97              0        0     1    0     0         0          0        0   \n",
       "98              0        0     0    1     0         0          0        0   \n",
       "99              0        0     0    0     0         0          0        0   \n",
       "100             1        0     0    1     0         0          0        0   \n",
       "101             0        0     0    1     0         1          0        0   \n",
       "102             0        0     0    1     0         0          0        0   \n",
       "103             0        0     0    1     0         1          0        0   \n",
       "104             0        0     0    1     0         1          0        0   \n",
       "105             0        0     0    1     0         0          0        0   \n",
       "106             0        0     1    0     0         1          0        0   \n",
       "107             0        0     0    1     1         1          0        0   \n",
       "108             0        0     0    1     1         1          0        0   \n",
       "109             0        0     0    1     0         1          0        0   \n",
       "\n",
       "     surprise  trust  \n",
       "0           0      0  \n",
       "1           0      0  \n",
       "2           0      0  \n",
       "3           0      0  \n",
       "4           0      0  \n",
       "5           0      0  \n",
       "6           0      0  \n",
       "7           0      0  \n",
       "8           0      0  \n",
       "9           0      0  \n",
       "10          0      0  \n",
       "11          0      0  \n",
       "12          0      0  \n",
       "13          0      0  \n",
       "14          0      0  \n",
       "15          0      0  \n",
       "16          0      0  \n",
       "17          0      0  \n",
       "18          0      0  \n",
       "19          0      0  \n",
       "20          0      0  \n",
       "21          0      0  \n",
       "22          0      0  \n",
       "23          0      0  \n",
       "24          0      0  \n",
       "25          0      0  \n",
       "26          0      0  \n",
       "27          0      0  \n",
       "28          0      0  \n",
       "29          0      0  \n",
       "30          0      0  \n",
       "31          0      0  \n",
       "32          0      0  \n",
       "33          0      0  \n",
       "34          0      0  \n",
       "35          0      0  \n",
       "36          0      0  \n",
       "37          0      0  \n",
       "38          0      0  \n",
       "39          0      0  \n",
       "40          0      0  \n",
       "41          0      0  \n",
       "42          0      0  \n",
       "43          0      0  \n",
       "44          0      0  \n",
       "45          0      0  \n",
       "46          1      0  \n",
       "47          0      1  \n",
       "48          0      0  \n",
       "49          0      0  \n",
       "50          0      0  \n",
       "51          0      0  \n",
       "52          0      0  \n",
       "53          0      0  \n",
       "54          0      1  \n",
       "55          0      0  \n",
       "56          0      1  \n",
       "57          0      0  \n",
       "58          0      0  \n",
       "59          0      0  \n",
       "60          1      1  \n",
       "61          0      0  \n",
       "62          0      0  \n",
       "63          0      0  \n",
       "64          0      0  \n",
       "65          0      1  \n",
       "66          0      0  \n",
       "67          0      0  \n",
       "68          0      1  \n",
       "69          1      0  \n",
       "70          0      0  \n",
       "71          0      0  \n",
       "72          0      0  \n",
       "73          0      0  \n",
       "74          0      0  \n",
       "75          0      0  \n",
       "76          0      0  \n",
       "77          0      0  \n",
       "78          0      0  \n",
       "79          0      0  \n",
       "80          0      1  \n",
       "81          0      0  \n",
       "82          0      0  \n",
       "83          0      0  \n",
       "84          0      1  \n",
       "85          0      0  \n",
       "86          0      0  \n",
       "87          0      0  \n",
       "88          0      0  \n",
       "89          0      0  \n",
       "90          1      0  \n",
       "91          1      0  \n",
       "92          1      0  \n",
       "93          1      0  \n",
       "94          1      0  \n",
       "95          1      0  \n",
       "96          1      0  \n",
       "97          1      0  \n",
       "98          1      0  \n",
       "99          1      0  \n",
       "100         0      1  \n",
       "101         0      1  \n",
       "102         0      1  \n",
       "103         0      1  \n",
       "104         0      1  \n",
       "105         0      1  \n",
       "106         0      1  \n",
       "107         0      1  \n",
       "108         0      1  \n",
       "109         0      1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.4: Drop rows with NaN values in the text column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling NaN Values in the Text Column\n",
    "\n",
    "#### Definition of NaN Values\n",
    "NaN stands for \"Not a Number\" and is used to represent missing or undefined values in a dataset. In the context of text data, NaN values indicate that a particular entry in the text column is missing or empty.\n",
    "\n",
    "#### Purpose of Dropping Rows with NaN Values\n",
    "Dropping rows with NaN values is an essential pre-processing step to ensure that the dataset is clean and complete. Working with incomplete data can lead to errors and unreliable results in text analysis and machine learning models. By removing rows with NaN values, we can:\n",
    "\n",
    "1. **Ensure Data Quality**: Removing incomplete data entries helps maintain the integrity and quality of the dataset.\n",
    "2. **Prevent Errors**: Many text processing functions and machine learning algorithms cannot handle NaN values and will raise errors if they encounter them.\n",
    "3. **Improve Model Performance**: Clean and complete data contributes to more accurate and reliable model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110\n"
     ]
    }
   ],
   "source": [
    "# Sample data without Droping Rows with NAN Values\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with NaN values in the text column\n",
    "data = data.dropna(subset=['Tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110\n"
     ]
    }
   ],
   "source": [
    "# Sample data After Droping Rows with NAN Values\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.5: Apply Data Cleaning\n",
    "Remove symbol and numbers \n",
    "Data in original format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                            Need a new outlet for #rage\n",
       "1      Arguing with these people doesn't work anyway,...\n",
       "2      When your sister is 19 and throws legitimate t...\n",
       "3      #HRmanagement must discourage the expediency f...\n",
       "4      It is too fucking bright &amp; too fucking hot...\n",
       "5      Don't join @BTCare they put the phone down on ...\n",
       "6      Don't be offended,\\nI'm just doing something t...\n",
       "7      #Azerbaijan #Baku Azerbaijan to prevent anothe...\n",
       "8      and i will strike down upon thee with great ve...\n",
       "9      Hey @PapaJohnsUK I've been charged ¬£40.24 on m...\n",
       "10     No quite sure how Craig Gordon's stayed on the...\n",
       "11     one month til someone's bday and i think it's ...\n",
       "12     Shriekfest is lining up VOLUNTEERS! Oct 6-9, o...\n",
       "13     This makes things easier and compact and less ...\n",
       "14     @StutteringGiant at least his character in fas...\n",
       "15     @rickygervais my first time in Slough so check...\n",
       "16     @ruthwalford95 you may be right, but since yea...\n",
       "17     New job training= too much meat eating! Oh wel...\n",
       "18     @ANI_news #Pakistan should stop cross border #...\n",
       "19     When you have 15 doe run the opposite side of ...\n",
       "20              Riggs dumb ass hell lolol  #LethalWeapon\n",
       "21     Metal keeps you young and spry and keeps your ...\n",
       "22     Farting in a hot are car when your windows don...\n",
       "23     Pro Tip: Go back to work when your kid reaches...\n",
       "24                 Feels grim not having your nails done\n",
       "25     @Meruna_  as a musician, I can tell you that m...\n",
       "26     @thomeagle Just to help maintain and boost our...\n",
       "27     I wonder if the #wolfcreek TV show is sponsore...\n",
       "28     I don't like pineapple I only eat them on pizz...\n",
       "29     @Bell @Bell_Support Cancelling home Fibe, Inte...\n",
       "30          I'm a walking ball of stress and anxiety lol\n",
       "31     I just killed a spider so big it sprayed spide...\n",
       "32     #heavyheart these last couple of days, who are...\n",
       "33                       The anxiety I have right nowüò≠üò≠üò≠\n",
       "34     @GuardianAus @Paul_Karp Maybe 49% support no #...\n",
       "35     @Mista_Nightmare i dont understand why u do vi...\n",
       "36     @wittyneeraj What is Shehla Rashid &amp; Kaniy...\n",
       "37           I have no clue where my charger is... #lost\n",
       "38              @LaurenBrierley2 sparkling water = death\n",
       "39     @jade0208 unfortunately the diet is still on, ...\n",
       "40     I'm so playful. lol I need somebody that'll jo...\n",
       "41     The Pats are awesome. Belichick is awesome ......\n",
       "42     Beginning the process to see if working is an ...\n",
       "43     I'm onto you, @Sargon_of_Akkad, I know you sec...\n",
       "44     @RadioX @ChrisMoyles wow. not heard this in fo...\n",
       "45     lost 11 lbs since I got married (eating health...\n",
       "46     @Aurena1701 @AngryOrchard he didn't sting me l...\n",
       "47          backed pats -2.5 10/11 just before #pleasing\n",
       "48                      GKN so lively as well, mad quick\n",
       "49     My morning started off amazing!! Hopefully the...\n",
       "50     @phil500 \\nSo sadden \\nSpunky a beautiful dog\\...\n",
       "51     also other ppl who i love a whole damn lot; er...\n",
       "52     @Singaholic121 Good morning, love! Happy first...\n",
       "53                              #GBBO can cheer me up ‚ò∫Ô∏è\n",
       "54     i've been rooting for him since the beginning ...\n",
       "55     I just want to let everyone out there know you...\n",
       "56         this girl at my new school is so pretty i am \n",
       "57                        @mcrichard awe that's adorable\n",
       "58     noah fence but i want a harley quinn or blake ...\n",
       "59     @Devilligan It's a beautifully sincere balanci...\n",
       "60     @jtkola @danmericaCNN she's still younger and ...\n",
       "61     When you make a great tasting shake and no wor...\n",
       "62     I think our defense here at USC is playing wel...\n",
       "63     In ever use to like smiling until I realized h...\n",
       "64     #2 complained then while his head and then cal...\n",
       "65     When @idinamenzel says she's releasing more #I...\n",
       "66     While we focus on issue of #IPCA @IHFOKids Ind...\n",
       "67     @AimiSyafiqahR go check up with your bf. He'll...\n",
       "68     @WSJNordics You make the world a more joyful p...\n",
       "69     The ecosystem is meant to break thru the wall ...\n",
       "70     After this news I'm supposed to be so damn hap...\n",
       "71     Not written for African-American\\n\\nNo refuge ...\n",
       "72     Once again the only thing on my feed is naay r...\n",
       "73     And she got all angry telling me 'but what wou...\n",
       "74     god, Facebook's design has started to remind m...\n",
       "75     @myaeggs I can't get a better look at her bc I...\n",
       "76     @RoWillFindYou Eric couldn't help but laugh, t...\n",
       "77     I miss doing nothing someone I care about and ...\n",
       "78                       feel really sad and down todayüòí\n",
       "79     Not sure that men can handle a woman that's go...\n",
       "80     When we give cheerfully and accept gratefully,...\n",
       "81                               The rage has died down.\n",
       "82     @XboxMAD @RobotBrush Ballmer will be furious. ...\n",
       "83     @simon_penn_r @AntisocialJW2 has turned into a...\n",
       "84     @hannah_2401 hannah stop being mournful and ch...\n",
       "85     @RaveenElexiis I fought a racist girl and she ...\n",
       "86          Don't wanna go to work but I want the money \n",
       "87     @luxbet Did you even give out any pizzas ? Ser...\n",
       "88     @jbanks88 offense can't score 3 redzone trips ...\n",
       "89     And I won't even get started with Hillary and ...\n",
       "90     You don't know what to expect by Brendon's vid...\n",
       "91                                     Are you serious??\n",
       "92     @samsteinhp stop the presses, @realDonaldTrump...\n",
       "93     @NHLstoreNYC I'm cheering for @TeamNA_WCH and ...\n",
       "94     @Trump_Videos she looks completely #rabid @rea...\n",
       "95     @vibaby @imEB y'all had crackheads in the isla...\n",
       "96     @Policy_Exchange A plus point, she won't have ...\n",
       "97                                            I'm in awe\n",
       "98     Vale! Vale! Sip sangria and taste tantalizing ...\n",
       "99     #tulsa - Police manufacture murder... Wonder w...\n",
       "100    @RVAGameBreak @GAHSBasketball @GAJagsFootball ...\n",
       "101    üòÇ - youre one of those whod bully me omg but i...\n",
       "102    #ukedchat A4 Just go outside (or to the gym ha...\n",
       "103    Zephaniah 3:17 He surely is rejoicing over us ...\n",
       "104                Refuse to let myself get discouraged.\n",
       "105    @AlaskaGurus @adventuretweets agreed! üòç an awe...\n",
       "106    Don't be #afraid of the space between your #dr...\n",
       "107    @GameGrumps THANK YOU SO MUCH FOR COMING TO DE...\n",
       "108    And by the way, Takeru's eyes sparkling while ...\n",
       "109    Accept the challenges so that you can feel the...\n",
       "Name: Tweet, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Tweet']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply function to remove symbols and numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Tweet'] = data['Tweet'].apply(remove_symbols_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See data again to see the implementation of removal of symbols and numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                             Need a new outlet for rage\n",
       "1      Arguing with these people doesnt work anyway t...\n",
       "2      When your sister is  and throws legitimate tem...\n",
       "3      HRmanagement must discourage the expediency fa...\n",
       "4      It is too fucking bright amp too fucking hot o...\n",
       "5      Dont join BTCare they put the phone down on yo...\n",
       "6      Dont be offendednIm just doing something thatn...\n",
       "7      Azerbaijan Baku Azerbaijan to prevent another ...\n",
       "8      and i will strike down upon thee with great ve...\n",
       "9      Hey PapaJohnsUK Ive been charged  on my credit...\n",
       "10     No quite sure how Craig Gordons stayed on the ...\n",
       "11     one month til someones bday and i think its ti...\n",
       "12     Shriekfest is lining up VOLUNTEERS Oct  only s...\n",
       "13     This makes things easier and compact and less ...\n",
       "14     StutteringGiant at least his character in fast...\n",
       "15     rickygervais my first time in Slough so checke...\n",
       "16     ruthwalford you may be right but since year th...\n",
       "17     New job training too much meat eating Oh well ...\n",
       "18     ANInews Pakistan should stop cross border terr...\n",
       "19     When you have  doe run the opposite side of you  \n",
       "20               Riggs dumb ass hell lolol  LethalWeapon\n",
       "21     Metal keeps you young and spry and keeps your ...\n",
       "22     Farting in a hot are car when your windows don...\n",
       "23     Pro Tip Go back to work when your kid reaches ...\n",
       "24                 Feels grim not having your nails done\n",
       "25     Meruna  as a musician I can tell you that more...\n",
       "26     thomeagle Just to help maintain and boost our ...\n",
       "27     I wonder if the wolfcreek TV show is sponsored...\n",
       "28     I dont like pineapple I only eat them on pizza...\n",
       "29     Bell BellSupport Cancelling home Fibe Internet...\n",
       "30           Im a walking ball of stress and anxiety lol\n",
       "31     I just killed a spider so big it sprayed spide...\n",
       "32     heavyheart these last couple of days who are t...\n",
       "33                          The anxiety I have right now\n",
       "34     GuardianAus PaulKarp Maybe  support no Muslim ...\n",
       "35     MistaNightmare i dont understand why u do vide...\n",
       "36     wittyneeraj What is Shehla Rashid amp Kaniyah ...\n",
       "37               I have no clue where my charger is lost\n",
       "38                 LaurenBrierley sparkling water  death\n",
       "39     jade unfortunately the diet is still on so the...\n",
       "40     Im so playful lol I need somebody thatll joke ...\n",
       "41     The Pats are awesome Belichick is awesome they...\n",
       "42     Beginning the process to see if working is an ...\n",
       "43     Im onto you SargonofAkkad I know you secretly ...\n",
       "44     RadioX ChrisMoyles wow not heard this in forev...\n",
       "45     lost  lbs since I got married eating healthy a...\n",
       "46     Aurena AngryOrchard he didnt sting me luckily ...\n",
       "47                    backed pats   just before pleasing\n",
       "48                       GKN so lively as well mad quick\n",
       "49     My morning started off amazing Hopefully the w...\n",
       "50     phil nSo sadden nSpunky a beautiful dognA sad ...\n",
       "51     also other ppl who i love a whole damn lot eri...\n",
       "52     Singaholic Good morning love Happy first day o...\n",
       "53                                 GBBO can cheer me up \n",
       "54      ive been rooting for him since the beginning  BB\n",
       "55     I just want to let everyone out there know you...\n",
       "56         this girl at my new school is so pretty i am \n",
       "57                          mcrichard awe thats adorable\n",
       "58     noah fence but i want a harley quinn or blake ...\n",
       "59     Devilligan Its a beautifully sincere balancing...\n",
       "60     jtkola danmericaCNN shes still younger and mor...\n",
       "61     When you make a great tasting shake and no wor...\n",
       "62     I think our defense here at USC is playing wel...\n",
       "63     In ever use to like smiling until I realized h...\n",
       "64      complained then while his head and then calle...\n",
       "65     When idinamenzel says shes releasing more Idin...\n",
       "66     While we focus on issue of IPCA IHFOKids Indul...\n",
       "67     AimiSyafiqahR go check up with your bf Hell gi...\n",
       "68     WSJNordics You make the world a more joyful pl...\n",
       "69     The ecosystem is meant to break thru the wall ...\n",
       "70     After this news Im supposed to be so damn happ...\n",
       "71     Not written for AfricanAmericannnNo refuge cou...\n",
       "72     Once again the only thing on my feed is naay r...\n",
       "73     And she got all angry telling me but what woul...\n",
       "74     god Facebooks design has started to remind me ...\n",
       "75     myaeggs I cant get a better look at her bc Im ...\n",
       "76     RoWillFindYou Eric couldnt help but laugh thou...\n",
       "77     I miss doing nothing someone I care about and ...\n",
       "78                        feel really sad and down today\n",
       "79     Not sure that men can handle a woman thats got...\n",
       "80     When we give cheerfully and accept gratefully ...\n",
       "81                                The rage has died down\n",
       "82     XboxMAD RobotBrush Ballmer will be furious Ano...\n",
       "83     simonpennr AntisocialJW has turned into a rath...\n",
       "84          hannah hannah stop being mournful and chill \n",
       "85     RaveenElexiis I fought a racist girl and she s...\n",
       "86           Dont wanna go to work but I want the money \n",
       "87     luxbet Did you even give out any pizzas  Serio...\n",
       "88     jbanks offense cant score  redzone trips no po...\n",
       "89     And I wont even get started with Hillary and h...\n",
       "90     You dont know what to expect by Brendons video...\n",
       "91                                       Are you serious\n",
       "92     samsteinhp stop the presses realDonaldTrump sa...\n",
       "93     NHLstoreNYC Im cheering for TeamNAWCH and Team...\n",
       "94     TrumpVideos she looks completely rabid realDon...\n",
       "95     vibaby imEB yall had crackheads in the islands...\n",
       "96     PolicyExchange A plus point she wont have to q...\n",
       "97                                             Im in awe\n",
       "98     Vale Vale Sip sangria and taste tantalizing ta...\n",
       "99     tulsa  Police manufacture murder Wonder why we...\n",
       "100    RVAGameBreak GAHSBasketball GAJagsFootball  nG...\n",
       "101      youre one of those whod bully me omg but it ...\n",
       "102    ukedchat A Just go outside or to the gym hall ...\n",
       "103    Zephaniah  He surely is rejoicing over us with...\n",
       "104                 Refuse to let myself get discouraged\n",
       "105    AlaskaGurus adventuretweets agreed  an awe to ...\n",
       "106    Dont be afraid of the space between your dream...\n",
       "107    GameGrumps THANK YOU SO MUCH FOR COMING TO DET...\n",
       "108    And by the way Takerus eyes sparkling while ea...\n",
       "109    Accept the challenges so that you can feel the...\n",
       "Name: Tweet, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Tweet']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply function to convert to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Tweet'] = data['Tweet'].apply(to_lowercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                             need a new outlet for rage\n",
       "1      arguing with these people doesnt work anyway t...\n",
       "2      when your sister is  and throws legitimate tem...\n",
       "3      hrmanagement must discourage the expediency fa...\n",
       "4      it is too fucking bright amp too fucking hot o...\n",
       "5      dont join btcare they put the phone down on yo...\n",
       "6      dont be offendednim just doing something thatn...\n",
       "7      azerbaijan baku azerbaijan to prevent another ...\n",
       "8      and i will strike down upon thee with great ve...\n",
       "9      hey papajohnsuk ive been charged  on my credit...\n",
       "10     no quite sure how craig gordons stayed on the ...\n",
       "11     one month til someones bday and i think its ti...\n",
       "12     shriekfest is lining up volunteers oct  only s...\n",
       "13     this makes things easier and compact and less ...\n",
       "14     stutteringgiant at least his character in fast...\n",
       "15     rickygervais my first time in slough so checke...\n",
       "16     ruthwalford you may be right but since year th...\n",
       "17     new job training too much meat eating oh well ...\n",
       "18     aninews pakistan should stop cross border terr...\n",
       "19     when you have  doe run the opposite side of you  \n",
       "20               riggs dumb ass hell lolol  lethalweapon\n",
       "21     metal keeps you young and spry and keeps your ...\n",
       "22     farting in a hot are car when your windows don...\n",
       "23     pro tip go back to work when your kid reaches ...\n",
       "24                 feels grim not having your nails done\n",
       "25     meruna  as a musician i can tell you that more...\n",
       "26     thomeagle just to help maintain and boost our ...\n",
       "27     i wonder if the wolfcreek tv show is sponsored...\n",
       "28     i dont like pineapple i only eat them on pizza...\n",
       "29     bell bellsupport cancelling home fibe internet...\n",
       "30           im a walking ball of stress and anxiety lol\n",
       "31     i just killed a spider so big it sprayed spide...\n",
       "32     heavyheart these last couple of days who are t...\n",
       "33                          the anxiety i have right now\n",
       "34     guardianaus paulkarp maybe  support no muslim ...\n",
       "35     mistanightmare i dont understand why u do vide...\n",
       "36     wittyneeraj what is shehla rashid amp kaniyah ...\n",
       "37               i have no clue where my charger is lost\n",
       "38                 laurenbrierley sparkling water  death\n",
       "39     jade unfortunately the diet is still on so the...\n",
       "40     im so playful lol i need somebody thatll joke ...\n",
       "41     the pats are awesome belichick is awesome they...\n",
       "42     beginning the process to see if working is an ...\n",
       "43     im onto you sargonofakkad i know you secretly ...\n",
       "44     radiox chrismoyles wow not heard this in forev...\n",
       "45     lost  lbs since i got married eating healthy a...\n",
       "46     aurena angryorchard he didnt sting me luckily ...\n",
       "47                    backed pats   just before pleasing\n",
       "48                       gkn so lively as well mad quick\n",
       "49     my morning started off amazing hopefully the w...\n",
       "50     phil nso sadden nspunky a beautiful dogna sad ...\n",
       "51     also other ppl who i love a whole damn lot eri...\n",
       "52     singaholic good morning love happy first day o...\n",
       "53                                 gbbo can cheer me up \n",
       "54      ive been rooting for him since the beginning  bb\n",
       "55     i just want to let everyone out there know you...\n",
       "56         this girl at my new school is so pretty i am \n",
       "57                          mcrichard awe thats adorable\n",
       "58     noah fence but i want a harley quinn or blake ...\n",
       "59     devilligan its a beautifully sincere balancing...\n",
       "60     jtkola danmericacnn shes still younger and mor...\n",
       "61     when you make a great tasting shake and no wor...\n",
       "62     i think our defense here at usc is playing wel...\n",
       "63     in ever use to like smiling until i realized h...\n",
       "64      complained then while his head and then calle...\n",
       "65     when idinamenzel says shes releasing more idin...\n",
       "66     while we focus on issue of ipca ihfokids indul...\n",
       "67     aimisyafiqahr go check up with your bf hell gi...\n",
       "68     wsjnordics you make the world a more joyful pl...\n",
       "69     the ecosystem is meant to break thru the wall ...\n",
       "70     after this news im supposed to be so damn happ...\n",
       "71     not written for africanamericannnno refuge cou...\n",
       "72     once again the only thing on my feed is naay r...\n",
       "73     and she got all angry telling me but what woul...\n",
       "74     god facebooks design has started to remind me ...\n",
       "75     myaeggs i cant get a better look at her bc im ...\n",
       "76     rowillfindyou eric couldnt help but laugh thou...\n",
       "77     i miss doing nothing someone i care about and ...\n",
       "78                        feel really sad and down today\n",
       "79     not sure that men can handle a woman thats got...\n",
       "80     when we give cheerfully and accept gratefully ...\n",
       "81                                the rage has died down\n",
       "82     xboxmad robotbrush ballmer will be furious ano...\n",
       "83     simonpennr antisocialjw has turned into a rath...\n",
       "84          hannah hannah stop being mournful and chill \n",
       "85     raveenelexiis i fought a racist girl and she s...\n",
       "86           dont wanna go to work but i want the money \n",
       "87     luxbet did you even give out any pizzas  serio...\n",
       "88     jbanks offense cant score  redzone trips no po...\n",
       "89     and i wont even get started with hillary and h...\n",
       "90     you dont know what to expect by brendons video...\n",
       "91                                       are you serious\n",
       "92     samsteinhp stop the presses realdonaldtrump sa...\n",
       "93     nhlstorenyc im cheering for teamnawch and team...\n",
       "94     trumpvideos she looks completely rabid realdon...\n",
       "95     vibaby imeb yall had crackheads in the islands...\n",
       "96     policyexchange a plus point she wont have to q...\n",
       "97                                             im in awe\n",
       "98     vale vale sip sangria and taste tantalizing ta...\n",
       "99     tulsa  police manufacture murder wonder why we...\n",
       "100    rvagamebreak gahsbasketball gajagsfootball  ng...\n",
       "101      youre one of those whod bully me omg but it ...\n",
       "102    ukedchat a just go outside or to the gym hall ...\n",
       "103    zephaniah  he surely is rejoicing over us with...\n",
       "104                 refuse to let myself get discouraged\n",
       "105    alaskagurus adventuretweets agreed  an awe to ...\n",
       "106    dont be afraid of the space between your dream...\n",
       "107    gamegrumps thank you so much for coming to det...\n",
       "108    and by the way takerus eyes sparkling while ea...\n",
       "109    accept the challenges so that you can feel the...\n",
       "Name: Tweet, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Tweet']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply function to remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Tweet'] = data['Tweet'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                   need new outlet rage\n",
       "1      arguing people doesnt work anyway threaten put...\n",
       "2      sister throws legitimate temper tantrums get a...\n",
       "3      hrmanagement must discourage expediency factor...\n",
       "4                 fucking bright amp fucking hot outside\n",
       "5      dont join btcare put phone talk rude taking mo...\n",
       "6      dont offendednim something thatngives life hur...\n",
       "7      azerbaijan baku azerbaijan prevent another arm...\n",
       "8      strike upon thee great vengeance furious anger...\n",
       "9      hey papajohnsuk ive charged credit card order ...\n",
       "10                  quite sure craig gordons stayed park\n",
       "11     one month til someones bday think time flaunt ...\n",
       "12     shriekfest lining volunteers oct serious inqui...\n",
       "13     makes things easier compact less fiery burdene...\n",
       "14     stutteringgiant least character fast furious p...\n",
       "15     rickygervais first time slough checked new sta...\n",
       "16     ruthwalford may right since year bad events be...\n",
       "17     new job training much meat eating oh well also...\n",
       "18     aninews pakistan stop cross border terrorism r...\n",
       "19                                 doe run opposite side\n",
       "20                riggs dumb ass hell lolol lethalweapon\n",
       "21     metal keeps young spry keeps hair luxuriousnny...\n",
       "22     farting hot car windows dont roll terrible gag...\n",
       "23     pro tip go back work kid reaches mos old stay ...\n",
       "24                                 feels grim nails done\n",
       "25     meruna musician tell people get discouraged le...\n",
       "26     thomeagle help maintain boost status world cla...\n",
       "27     wonder wolfcreek tv show sponsored antitourist...\n",
       "28     dont like pineapple eat pizza lose sting get c...\n",
       "29     bell bellsupport cancelling home fibe internet...\n",
       "30                    im walking ball stress anxiety lol\n",
       "31     killed spider big sprayed spider guts like hor...\n",
       "32     heavyheart last couple days cause fear losing ...\n",
       "33                                         anxiety right\n",
       "34     guardianaus paulkarp maybe support muslim immi...\n",
       "35     mistanightmare dont understand u videos every ...\n",
       "36     wittyneeraj shehla rashid amp kaniyah kumar sa...\n",
       "37                                     clue charger lost\n",
       "38                  laurenbrierley sparkling water death\n",
       "39     jade unfortunately diet still wait till friday...\n",
       "40     im playful lol need somebody thatll joke day l...\n",
       "41              pats awesome belichick awesome awestruck\n",
       "42     beginning process see working option mentalhea...\n",
       "43     im onto sargonofakkad know secretly pine mascu...\n",
       "44     radiox chrismoyles wow heard forever random gr...\n",
       "45     lost lbs since got married eating healthy amp ...\n",
       "46     aurena angryorchard didnt sting luckily flew h...\n",
       "47                                  backed pats pleasing\n",
       "48                             gkn lively well mad quick\n",
       "49     morning started amazing hopefully whole day go...\n",
       "50     phil nso sadden nspunky beautiful dogna sad st...\n",
       "51     also ppl love whole damn lot erica joanna tiff...\n",
       "52     singaholic good morning love happy first day f...\n",
       "53                                            gbbo cheer\n",
       "54                        ive rooting since beginning bb\n",
       "55     want let everyone know wonderful people beyour...\n",
       "56                                girl new school pretty\n",
       "57                          mcrichard awe thats adorable\n",
       "58      noah fence want harley quinn blake lively layout\n",
       "59     devilligan beautifully sincere balancing act g...\n",
       "60     jtkola danmericacnn shes still younger spry trump\n",
       "61     make great tasting shake words describe herbal...\n",
       "62     think defense usc playing well need fix things...\n",
       "63     ever use like smiling realized good teeth look...\n",
       "64     complained head called despair gods mercy sins...\n",
       "65     idinamenzel says shes releasing idinaparty tic...\n",
       "66     focus issue ipca ihfokids indulges intimidatio...\n",
       "67     aimisyafiqahr go check bf hell give u strength...\n",
       "68         wsjnordics make world joyful place thenicebot\n",
       "69     ecosystem meant break thru wall apprehension e...\n",
       "70         news im supposed damn happy rejoicing im like\n",
       "71     written africanamericannnno refuge could save ...\n",
       "72     thing feed naay raging something brother filli...\n",
       "73     got angry telling would year old guy looking g...\n",
       "74     god facebooks design started remind myspace he...\n",
       "75     myaeggs cant get better look bc im shy make ey...\n",
       "76     rowillfindyou eric couldnt help laugh though m...\n",
       "77     miss nothing someone care attacking face kisse...\n",
       "78                                 feel really sad today\n",
       "79     sure men handle woman thats got crap together ...\n",
       "80     give cheerfully accept gratefully everyone ble...\n",
       "81                                             rage died\n",
       "82     xboxmad robotbrush ballmer furious another del...\n",
       "83     simonpennr antisocialjw turned rather fiery gg...\n",
       "84                     hannah hannah stop mournful chill\n",
       "85     raveenelexiis fought racist girl stayed gavc g...\n",
       "86                         dont wanna go work want money\n",
       "87     luxbet even give pizzas serious fail hungry fu...\n",
       "88     jbanks offense cant score redzone trips points...\n",
       "89     wont even get started hillary fancy fundraiser...\n",
       "90     dont know expect brendons video lmao la devote...\n",
       "91                                               serious\n",
       "92     samsteinhp stop presses realdonaldtrump saidpr...\n",
       "93     nhlstorenyc im cheering teamnawch teameurwch n...\n",
       "94     trumpvideos looks completely rabid realdonaldt...\n",
       "95     vibaby imeb yall crackheads islands dont know ...\n",
       "96     policyexchange plus point wont queue loos plus...\n",
       "97                                                im awe\n",
       "98     vale vale sip sangria taste tantalizing tapas ...\n",
       "99     tulsa police manufacture murder wonder carry b...\n",
       "100    rvagamebreak gahsbasketball gajagsfootball ngo...\n",
       "101    youre one whod bully omg banter could laugh la...\n",
       "102    ukedchat go outside gym hall play n education ...\n",
       "103    zephaniah surely rejoicing us singingsuch roma...\n",
       "104                           refuse let get discouraged\n",
       "105    alaskagurus adventuretweets agreed awe meet be...\n",
       "106          dont afraid space dreams reality dream make\n",
       "107    gamegrumps thank much coming detroit im going ...\n",
       "108    way takerus eyes sparkling eating like cutest ...\n",
       "109    accept challenges feel exhilaration victory fo...\n",
       "Name: Tweet, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Tweet']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Following function contains the all three tasks for the sake of simplicity we dividied it in three parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the 'tweet' column\n",
    "data['Tweet'] = data['Tweet'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.6: Data After Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Sentiment Analysis Data After Preprocessing:\n",
      "=================================================\n",
      "\n",
      "              ID                                              Tweet  anger  \\\n",
      "0  2017-En-10331                               need new outlet rage      1   \n",
      "1  2017-En-21898  arguing people doesnt work anyway threaten put...      1   \n",
      "2  2017-En-10398  sister throws legitimate temper tantrums get a...      1   \n",
      "3  2017-En-40975  hrmanagement must discourage expediency factor...      1   \n",
      "4  2017-En-30811             fucking bright amp fucking hot outside      1   \n",
      "\n",
      "   anticipation  disgust  fear  joy  love  optimism  pessimism  sadness  \\\n",
      "0             0        1     0    0     0         0          0        0   \n",
      "1             0        1     1    0     0         0          0        0   \n",
      "2             0        1     0    0     0         0          0        0   \n",
      "3             0        1     0    0     0         1          0        0   \n",
      "4             0        1     0    0     0         0          0        0   \n",
      "\n",
      "   surprise  trust  \n",
      "0         0      0  \n",
      "1         0      0  \n",
      "2         0      0  \n",
      "3         0      0  \n",
      "4         0      0  \n",
      "                ID                                              Tweet  anger  \\\n",
      "105  2017-En-21181  alaskagurus adventuretweets agreed awe meet be...      0   \n",
      "106  2017-En-21742        dont afraid space dreams reality dream make      0   \n",
      "107  2017-En-30853  gamegrumps thank much coming detroit im going ...      0   \n",
      "108  2017-En-30150  way takerus eyes sparkling eating like cutest ...      0   \n",
      "109  2017-En-30283  accept challenges feel exhilaration victory fo...      0   \n",
      "\n",
      "     anticipation  disgust  fear  joy  love  optimism  pessimism  sadness  \\\n",
      "105             0        0     0    1     0         0          0        0   \n",
      "106             0        0     1    0     0         1          0        0   \n",
      "107             0        0     0    1     1         1          0        0   \n",
      "108             0        0     0    1     1         1          0        0   \n",
      "109             0        0     0    1     0         1          0        0   \n",
      "\n",
      "     surprise  trust  \n",
      "105         0      1  \n",
      "106         0      1  \n",
      "107         0      1  \n",
      "108         0      1  \n",
      "109         0      1  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n\\nSentiment Analysis Data After Preprocessing:\")\n",
    "print(\"=================================================\\n\")\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "\n",
    "print(data.head())\n",
    "print(data.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'Tweet', 'anger', 'anticipation', 'disgust', 'fear', 'joy',\n",
       "       'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.7: Saving Cleaned Data as Seperate CSV File\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"cleaned_sample_tweets_emotions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'Tweet', 'anger', 'anticipation', 'disgust', 'fear', 'joy',\n",
       "       'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Splitting Sample Data into Training Data and Testing Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split\n",
    "\n",
    "#### Purpose of Train-Test Split\n",
    "\n",
    "Splitting the dataset into training and testing sets is a crucial step in the machine learning pipeline. It allows us to train our model on one subset of the data and evaluate its performance on another, unseen subset. This helps in assessing the model's ability to generalize to new data.\n",
    "\n",
    "#### Why Split the Data First?\n",
    "\n",
    "Before we can train a machine learning model, we need to split the data into training and testing sets. This ensures that we can evaluate the model's performance on data it hasn't seen during training. Additionally, we need to train the vectorizer (such as TF-IDF) on the training data to convert text to numerical features. Training the vectorizer on the training data ensures that it learns the vocabulary and importance of terms from the training set only, preventing any data leakage from the test set.\n",
    "\n",
    "### TF-IDF Vectorization\n",
    "\n",
    "#### Definition of TF-IDF\n",
    "\n",
    "TF-IDF stands for Term Frequency-Inverse Document Frequency. It is a statistical measure used to evaluate the importance of a word in a document relative to a collection of documents (corpus). The TF-IDF score increases proportionally to the number of times a word appears in a document but is offset by the frequency of the word in the corpus.\n",
    "\n",
    "- **Term Frequency (TF)**: Measures how frequently a term occurs in a document. \n",
    "  - TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)\n",
    "\n",
    "- **Inverse Document Frequency (IDF)**: Measures how important a term is. \n",
    "  - IDF(t) = log_e(Total number of documents / Number of documents with term t in it)\n",
    "\n",
    "- **TF-IDF**: The product of TF and IDF.\n",
    "  - TF-IDF(t) = TF(t) * IDF(t)\n",
    "\n",
    "#### Reason for Selecting TF-IDF\n",
    "\n",
    "TF-IDF is selected for vectorizing the text data because it not only considers the frequency of words within a document (like Term Frequency) but also adjusts for the fact that some words are generally more common than others (Inverse Document Frequency). This helps in highlighting the more meaningful words in a document and downplaying the less informative ones. \n",
    "\n",
    "Using TF-IDF allows us to convert text data into numerical features that can be used to train machine learning models. This vectorization is essential for applying algorithms that require numerical input.\n",
    "\n",
    "For more details on TF-IDF, you can refer to the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Data into Features and Labels\n",
    "\n",
    "In this step, we separate our data into features (X) and labels (y). The features (X) are the input data that the model will learn from, while the labels (y) are the target values that we want to predict.\n",
    "\n",
    "```python\n",
    "X = data['tweet']\n",
    "y = data['sentiment_type']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (88, 673)\n",
      "X_test shape: (22, 673)\n",
      "y_train shape: (88, 11)\n",
      "y_test shape: (22, 11)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('cleaned_sample_tweets_emotions.csv')\n",
    "# Separate features and labels\n",
    "X = data['Tweet']\n",
    "y = data.drop(columns=['ID', 'Tweet'])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature extraction using TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Check the shapes to ensure correct splits and transformations\n",
    "print(\"X_train shape:\", X_train_tfidf.shape)\n",
    "print(\"X_test shape:\", X_test_tfidf.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of Parameters\n",
    "\n",
    "- `train_test_split` is a function from the `sklearn.model_selection` module that splits the data into training and testing sets.\n",
    "- `X_train` and `y_train` are the features and labels for the training set.\n",
    "- `X_test` and `y_test` are the features and labels for the testing set.\n",
    "\n",
    "#### Parameters:\n",
    "\n",
    "1. **X and y**:\n",
    "   - `X`: The features or input data, which in this case is the 'tweet' column containing the text data.\n",
    "   - `y`: The labels or target values, which in this case is the 'sentiment_type' column indicating the sentiment of each tweet.\n",
    "\n",
    "2. **test_size**:\n",
    "   - `test_size=0.2` specifies that 20% of the data should be used as the testing set, and the remaining 80% will be used as the training set.\n",
    "   - The `test_size` parameter determines the proportion of the dataset to include in the test split. In this example, 0.2 means that 20% of the data will be allocated to the test set, and 80% will be allocated to the training set.\n",
    "   - Example: If the dataset contains 1000 samples, `test_size=0.2` means 800 samples will be used for training, and 200 samples will be used for testing.\n",
    "\n",
    "3. **random_state**:\n",
    "   - `random_state=42` ensures that the split is reproducible. Using the same `random_state` value will always result in the same split.\n",
    "   - The `random_state` parameter controls the shuffling applied to the data before applying the split. Providing a fixed value (e.g., 42) ensures that you get the same train-test split every time you run the code. This is useful for reproducibility and debugging.\n",
    "   - Example: If you set `random_state=42`, the data will be shuffled in the same way each time you run the code, resulting in the same training and testing sets.\n",
    "\n",
    "By performing this split, we ensure that we have separate datasets for training and evaluating our model, which is crucial for assessing the model's ability to generalize to new data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65     idinamenzel says shes releasing idinaparty tic...\n",
       "26     thomeagle help maintain boost status world cla...\n",
       "22     farting hot car windows dont roll terrible gag...\n",
       "31     killed spider big sprayed spider guts like hor...\n",
       "47                                  backed pats pleasing\n",
       "76     rowillfindyou eric couldnt help laugh though m...\n",
       "15     rickygervais first time slough checked new sta...\n",
       "44     radiox chrismoyles wow heard forever random gr...\n",
       "89     wont even get started hillary fancy fundraiser...\n",
       "90     dont know expect brendons video lmao la devote...\n",
       "9      hey papajohnsuk ive charged credit card order ...\n",
       "33                                         anxiety right\n",
       "55     want let everyone know wonderful people beyour...\n",
       "69     ecosystem meant break thru wall apprehension e...\n",
       "28     dont like pineapple eat pizza lose sting get c...\n",
       "40     im playful lol need somebody thatll joke day l...\n",
       "5      dont join btcare put phone talk rude taking mo...\n",
       "53                                            gbbo cheer\n",
       "62     think defense usc playing well need fix things...\n",
       "39     jade unfortunately diet still wait till friday...\n",
       "35     mistanightmare dont understand u videos every ...\n",
       "16     ruthwalford may right since year bad events be...\n",
       "104                           refuse let get discouraged\n",
       "34     guardianaus paulkarp maybe support muslim immi...\n",
       "67     aimisyafiqahr go check bf hell give u strength...\n",
       "7      azerbaijan baku azerbaijan prevent another arm...\n",
       "43     im onto sargonofakkad know secretly pine mascu...\n",
       "66     focus issue ipca ihfokids indulges intimidatio...\n",
       "73     got angry telling would year old guy looking g...\n",
       "27     wonder wolfcreek tv show sponsored antitourist...\n",
       "19                                 doe run opposite side\n",
       "88     jbanks offense cant score redzone trips points...\n",
       "93     nhlstorenyc im cheering teamnawch teameurwch n...\n",
       "25     meruna musician tell people get discouraged le...\n",
       "8      strike upon thee great vengeance furious anger...\n",
       "101    youre one whod bully omg banter could laugh la...\n",
       "49     morning started amazing hopefully whole day go...\n",
       "13     makes things easier compact less fiery burdene...\n",
       "77     miss nothing someone care attacking face kisse...\n",
       "24                                 feels grim nails done\n",
       "3      hrmanagement must discourage expediency factor...\n",
       "17     new job training much meat eating oh well also...\n",
       "38                  laurenbrierley sparkling water death\n",
       "85     raveenelexiis fought racist girl stayed gavc g...\n",
       "6      dont offendednim something thatngives life hur...\n",
       "105    alaskagurus adventuretweets agreed awe meet be...\n",
       "95     vibaby imeb yall crackheads islands dont know ...\n",
       "91                                               serious\n",
       "54                        ive rooting since beginning bb\n",
       "50     phil nso sadden nspunky beautiful dogna sad st...\n",
       "98     vale vale sip sangria taste tantalizing tapas ...\n",
       "46     aurena angryorchard didnt sting luckily flew h...\n",
       "83     simonpennr antisocialjw turned rather fiery gg...\n",
       "61     make great tasting shake words describe herbal...\n",
       "106          dont afraid space dreams reality dream make\n",
       "100    rvagamebreak gahsbasketball gajagsfootball ngo...\n",
       "41              pats awesome belichick awesome awestruck\n",
       "58      noah fence want harley quinn blake lively layout\n",
       "48                             gkn lively well mad quick\n",
       "94     trumpvideos looks completely rabid realdonaldt...\n",
       "57                          mcrichard awe thats adorable\n",
       "75     myaeggs cant get better look bc im shy make ey...\n",
       "32     heavyheart last couple days cause fear losing ...\n",
       "103    zephaniah surely rejoicing us singingsuch roma...\n",
       "59     devilligan beautifully sincere balancing act g...\n",
       "63     ever use like smiling realized good teeth look...\n",
       "107    gamegrumps thank much coming detroit im going ...\n",
       "37                                     clue charger lost\n",
       "29     bell bellsupport cancelling home fibe internet...\n",
       "97                                                im awe\n",
       "1      arguing people doesnt work anyway threaten put...\n",
       "52     singaholic good morning love happy first day f...\n",
       "21     metal keeps young spry keeps hair luxuriousnny...\n",
       "2      sister throws legitimate temper tantrums get a...\n",
       "23     pro tip go back work kid reaches mos old stay ...\n",
       "87     luxbet even give pizzas serious fail hungry fu...\n",
       "99     tulsa police manufacture murder wonder carry b...\n",
       "74     god facebooks design started remind myspace he...\n",
       "86                         dont wanna go work want money\n",
       "82     xboxmad robotbrush ballmer furious another del...\n",
       "109    accept challenges feel exhilaration victory fo...\n",
       "20                riggs dumb ass hell lolol lethalweapon\n",
       "60     jtkola danmericacnn shes still younger spry trump\n",
       "71     written africanamericannnno refuge could save ...\n",
       "14     stutteringgiant least character fast furious p...\n",
       "92     samsteinhp stop presses realdonaldtrump saidpr...\n",
       "51     also ppl love whole damn lot erica joanna tiff...\n",
       "102    ukedchat go outside gym hall play n education ...\n",
       "Name: Tweet, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78                                 feel really sad today\n",
       "10                  quite sure craig gordons stayed park\n",
       "4                 fucking bright amp fucking hot outside\n",
       "84                     hannah hannah stop mournful chill\n",
       "64     complained head called despair gods mercy sins...\n",
       "68         wsjnordics make world joyful place thenicebot\n",
       "30                    im walking ball stress anxiety lol\n",
       "45     lost lbs since got married eating healthy amp ...\n",
       "96     policyexchange plus point wont queue loos plus...\n",
       "11     one month til someones bday think time flaunt ...\n",
       "79     sure men handle woman thats got crap together ...\n",
       "80     give cheerfully accept gratefully everyone ble...\n",
       "0                                   need new outlet rage\n",
       "81                                             rage died\n",
       "18     aninews pakistan stop cross border terrorism r...\n",
       "70         news im supposed damn happy rejoicing im like\n",
       "56                                girl new school pretty\n",
       "72     thing feed naay raging something brother filli...\n",
       "108    way takerus eyes sparkling eating like cutest ...\n",
       "42     beginning process see working option mentalhea...\n",
       "12     shriekfest lining volunteers oct serious inqui...\n",
       "36     wittyneeraj shehla rashid amp kaniyah kumar sa...\n",
       "Name: Tweet, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.1: Feature Extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorization\n",
    "\n",
    "In this step, we use the `TfidfVectorizer` from the `sklearn.feature_extraction.text` module to convert the text data into numerical features based on the TF-IDF (Term Frequency-Inverse Document Frequency) representation. This helps in capturing the importance of words in the context of the documents they appear in.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the Vectorizer on the Training Data\n",
    "\n",
    "Before we can transform our text data into TF-IDF features, we need to fit the `TfidfVectorizer` on the training data. This step is crucial as it allows the vectorizer to learn the vocabulary and the importance of each term based on the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"‚ñ∏\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"‚ñæ\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer(max_features=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;TfidfVectorizer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\">?<span>Documentation for TfidfVectorizer</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>TfidfVectorizer(max_features=10)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "TfidfVectorizer(max_features=10)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the vectorizer on the training data\n",
    "vectorizer.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the Training Data\n",
    "\n",
    "After fitting the `TfidfVectorizer` on the training data, the next step is to transform the training text data into a TF-IDF feature matrix. This matrix will be used to train our machine learning model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the training data\n",
    "X_train_tfidf = vectorizer.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the TF-IDF Sparse Matrix to a DataFrame\n",
    "\n",
    "After transforming the text data into a TF-IDF feature matrix, we convert this sparse matrix into a DataFrame. This makes it easier to inspect and manipulate the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF Features DataFrame:\n",
      "   day  dont  get   go  got   im  know  like  make  want\n",
      "0  0.0   0.0  0.0  0.0  0.0  0.0   0.0   0.0   0.0   0.0\n",
      "1  0.0   0.0  0.0  0.0  0.0  0.0   0.0   0.0   0.0   0.0\n",
      "2  0.0   1.0  0.0  0.0  0.0  0.0   0.0   0.0   0.0   0.0\n",
      "3  0.0   0.0  0.0  0.0  0.0  0.0   0.0   1.0   0.0   0.0\n",
      "4  0.0   0.0  0.0  0.0  0.0  0.0   0.0   0.0   0.0   0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convert the TF-IDF sparse matrix to a DataFrame\n",
    "X_train_tfidf_df = pd.DataFrame(X_train_tfidf.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "print(\"\\nTF-IDF Features DataFrame:\")\n",
    "print(X_train_tfidf_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming and Converting the Test Data\n",
    "\n",
    "These steps are similar to the ones we performed on the training data, but now we are applying them to the test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF Features DataFrame:\n",
      "   day  dont  get   go  got   im  know  like  make  want\n",
      "0  0.0   0.0  0.0  0.0  0.0  0.0   0.0   0.0   0.0   0.0\n",
      "1  0.0   0.0  0.0  0.0  0.0  0.0   0.0   0.0   0.0   0.0\n",
      "2  0.0   0.0  0.0  0.0  0.0  0.0   0.0   0.0   0.0   0.0\n",
      "3  0.0   0.0  0.0  0.0  0.0  0.0   0.0   0.0   0.0   0.0\n",
      "4  0.0   0.0  0.0  1.0  0.0  0.0   0.0   0.0   0.0   0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Transform the training data\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Convert the TF-IDF sparse matrix to a DataFrame\n",
    "X_test_tfidf_df = pd.DataFrame(X_test_tfidf.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "print(\"\\nTF-IDF Features DataFrame:\")\n",
    "print(X_test_tfidf_df.head())\n",
    "\n",
    "#to do Parameter tuninng we are not clear how many features are optimal "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5 : Label Encoding does not required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Execute the Training Phase \n",
    "## Step 6.1: Training Data and Testing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6.2: Train the model\n",
    "\n",
    "### Training the Model\n",
    "\n",
    "In this step, we train a machine learning model using the TF-IDF features from the training data.\n",
    "\n",
    "`MultinomialNB()`: This initializes a Multinomial Naive Bayes classifier, which is well-suited for classification with discrete features like word counts or TF-IDF scores.\n",
    "`model.fit(X_train_tfidf, y_train)`: This method trains the Multinomial Naive Bayes model using the TF-IDF features (X_train_tfidf) and the corresponding labels (y_train). The model learns the relationship between the features and the labels, which will later be used to make predictions on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"‚ñ∏\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"‚ñæ\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultiOutputClassifier(estimator=LogisticRegression())</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;MultiOutputClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.multioutput.MultiOutputClassifier.html\">?<span>Documentation for MultiOutputClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>MultiOutputClassifier(estimator=LogisticRegression())</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">estimator: LogisticRegression</label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression()</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;LogisticRegression<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression()</pre></div> </div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultiOutputClassifier(estimator=LogisticRegression())"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model\n",
    "model = MultiOutputClassifier(LogisticRegression())\n",
    "model.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6.3: Save the trained model\n",
    "\n",
    "After training the model, it's important to save both the trained model and the TF-IDF vectorizer to disk. This allows us to reuse them later without retraining, which saves time and computational resources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tfidf_vectorizer.pkl']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model to disk\n",
    "joblib.dump(model, 'multi_label_model.pkl')\n",
    "\n",
    "# Save the vectorizer to disk\n",
    "joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Execute the Testing Phase \n",
    "## Step 7.1: Load the Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from 'multi_label_model.pkl'\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "*------------------- LOAD_SAVED_MODEL --------------------------*\n",
    "|         Function: load()                                      |\n",
    "|               Purpose: Method to Load Previously Saved Model  |\n",
    "|         Arguments:                                            |\n",
    "|               Model: Trained Model                            |\n",
    "|         Return:                                               |\n",
    "|               File: Saved Model will be Loaded in Memory      |\n",
    "*---------------------------------------------------------------*\n",
    "'''\n",
    "\n",
    "import joblib\n",
    "\n",
    "# Load the model from disk\n",
    "loaded_model = joblib.load('multi_label_model.pkl')\n",
    "\n",
    "# Load the vectorizer from disk\n",
    "loaded_vectorizer = joblib.load('tfidf_vectorizer.pkl')\n",
    "\n",
    "print(f\"Model loaded from 'multi_label_model.pkl'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7.2: Evaluate the Machine Learning Model\n",
    "\n",
    "### Transforming Test Data and Evaluating the Loaded Model\n",
    "\n",
    "Transform the test data using the loaded vectorizer: This step uses the loaded TF-IDF vectorizer to transform the test data (`X_test`) into the same TF-IDF feature matrix format as used during training.\n",
    "\n",
    "`X_test_tfidf_loaded = loaded_vectorizer.transform(X_test)`: This transforms the test text data into a TF-IDF feature matrix using the loaded vectorizer.\n",
    "\n",
    "Evaluate the loaded model: This step uses the loaded model to make predictions on the transformed test data and then evaluates the model's performance.\n",
    "\n",
    "`y_pred_loaded = loaded_model.predict(X_test_tfidf_loaded)`: This uses the loaded model to predict the labels for the transformed test data.\n",
    "`accuracy_loaded = accuracy_score(y_test, y_pred_loaded)`: This calculates the accuracy of the model's predictions by comparing the predicted labels (`y_pred_loaded`) with the true labels (`y_test`).\n",
    "`report_loaded = classification_report(y_test, y_pred_loaded)`: This generates a detailed classification report, which includes precision, recall, and F1-score for each class.\n",
    "\n",
    "Print the results: This step prints the accuracy and the classification report to provide a summary of the model's performance on the test data.\n",
    "\n",
    "`print(f\"Accuracy: {accuracy_loaded}\")`: This prints the accuracy of the model.\n",
    "`print(\"Classification Report:\")`: This prints a header for the classification report.\n",
    "`print(report_loaded)`: This prints the detailed classification report, providing insights into the model's performance across different classes.\n",
    "\n",
    "By transforming the test data and evaluating the loaded model, we can assess the model's ability to generalize to new, unseen data and ensure that it performs as expected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         7\n",
      "           1       0.00      0.00      0.00         5\n",
      "           2       0.00      0.00      0.00         8\n",
      "           3       0.00      0.00      0.00         4\n",
      "           4       0.50      0.11      0.18         9\n",
      "           5       0.00      0.00      0.00         3\n",
      "           6       1.00      0.14      0.25         7\n",
      "           7       0.00      0.00      0.00         4\n",
      "           8       0.00      0.00      0.00         7\n",
      "           9       0.00      0.00      0.00         1\n",
      "          10       0.00      0.00      0.00         5\n",
      "\n",
      "   micro avg       0.67      0.03      0.06        60\n",
      "   macro avg       0.14      0.02      0.04        60\n",
      "weighted avg       0.19      0.03      0.06        60\n",
      " samples avg       0.05      0.02      0.03        60\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Transform the test data using the loaded vectorizer\n",
    "X_test_tfidf_loaded = loaded_vectorizer.transform(X_test)\n",
    "\n",
    "# Evaluate the loaded model\n",
    "y_pred_loaded = loaded_model.predict(X_test_tfidf_loaded)\n",
    "accuracy_loaded = accuracy_score(y_test, y_pred_loaded)\n",
    "report_loaded = classification_report(y_test, y_pred_loaded)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_loaded}\")\n",
    "print(\"Classification Report:\")\n",
    "print(report_loaded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: Execute the Application Phase\n",
    "## Step 8.1: Take Input from User, Preprocess it \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take user input\n",
    "user_input = input(\"Please enter your text: \").strip()\n",
    "\n",
    "# Preprocess the user input\n",
    "def preprocess_user_input(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
    "    return text\n",
    "\n",
    "cleaned_input = preprocess_user_input(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i lost my dog'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_input\n",
    "# If they are successful, we can rest assured that the COVID-19 best vaccine will not irritate our eyes this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lost dog'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8.2: Load the Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the vectorizer and the model (ensure these are the same as used during training)\n",
    "vectorizer = joblib.load('tfidf_vectorizer.pkl')\n",
    "model = joblib.load('multi_label_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8.3: Transform the user input using the loaded vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF Features DataFrame for User Input:\n",
      "   day  dont  get   go  got   im  know  like  make  want\n",
      "0  0.0   0.0  0.0  0.0  0.0  0.0   0.0   0.0   0.0   0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Transform the cleaned input using the vectorizer\n",
    "X_test_tfidf = vectorizer.transform([cleaned_input])  # Wrap the cleaned input in a list\n",
    "\n",
    "# Convert the TF-IDF sparse matrix to a DataFrame\n",
    "X_test_tfidf_df = pd.DataFrame(X_test_tfidf.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "print(\"\\nTF-IDF Features DataFrame for User Input:\")\n",
    "print(X_test_tfidf_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the user input using the loaded vectorizer\n",
    "user_input_tfidf = vectorizer.transform([cleaned_input])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8.4: Predict the sentiment of the user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted sentiments for the input 'i lost my dog' are: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Python312\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Python312\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Python312\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Python312\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Python312\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Python312\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Python312\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Python312\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Python312\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Python312\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Predict the sentiment of the user input\n",
    "user_prediction = model.predict(X_test_tfidf_df)\n",
    "\n",
    "# Define a function to get sentiment labels\n",
    "def get_sentiments(predictions, labels):\n",
    "    sentiments = []\n",
    "    for pred, label in zip(predictions, labels):\n",
    "        if pred == 1:\n",
    "            sentiments.append(label)\n",
    "    return sentiments\n",
    "\n",
    "# Assuming you have a user input to test\n",
    "user_input_tfidf = vectorizer.transform([user_input])\n",
    "\n",
    "# Predict the sentiment of the user input\n",
    "user_prediction = model.predict(user_input_tfidf)\n",
    "\n",
    "# Output the predictions\n",
    "labels = y.columns  # Assuming y.columns contains the labels\n",
    "predicted_sentiments = get_sentiments(user_prediction[0], labels)\n",
    "print(f\"The predicted sentiments for the input '{user_input}' are: {', '.join(predicted_sentiments) if predicted_sentiments else 'None'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9: Execute the Feedback Phase\n",
    "## A Two-Step Process\n",
    "### Step 01: After some time, take Feedback from\n",
    "    o\tDomain Experts and Users on deployed Titanic Passenger Survival Prediction System\n",
    "### Step 02: Make a List of Possible Improvements based on Feedback received"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "todo gender identificaton form text\n",
    "\n",
    "muti clas age group identification from text, emotion redectipn fronm text. mahy by personalityh type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 10: Improve Model based on Feedback\n",
    "### There is Always Room for Improvement\n",
    "### Based on Feedback from Domain Experts and Users\n",
    "    o\tImprove your Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Todo Task\n",
    "\n",
    "## Choose a dataset from the following links and repeat the processes mentioned in this notebook:\n",
    "The first dataset is compulsory, while the others are provided for additional practice.\n",
    "\n",
    "1. **[Spam Email Dataset](https://www.kaggle.com/datasets/jackksoncsie/spam-email-dataset)** (Compulsory)\n",
    "2. [COVID-19 NLP Text Classification](https://www.kaggle.com/datasets/datatattle/covid-19-nlp-text-classification) (For practice)\n",
    "3. [Fake News Detection](https://www.kaggle.com/datasets/vishakhdapat/fake-news-detection) (For practice)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<center> <h3 style=\"color:blue\">==========================================================</h3> </center>\n",
    "<center> <h3 style=\"color:green\">JAZAK ALLAH KHAIR</h3> \n",
    "<center> <h3 style=\"color:blue\">==========================================================</h3> </center>\n",
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1844c191",
   "metadata": {},
   "source": [
    "# Multi-Label Classification with Sampled Emotion Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883a7565",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "\n",
    "# Load the sampled CSV file\n",
    "sampled_file_path = 'sampled_emotion_data.csv'\n",
    "df = pd.read_csv(sampled_file_path)\n",
    "\n",
    "# Separate features and labels\n",
    "X = df['Tweet']\n",
    "y = df.drop(columns=['ID', 'Tweet'])\n",
    "\n",
    "# Feature extraction using TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_tfidf = vectorizer.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the multi-label classification model\n",
    "model = MultiOutputClassifier(LogisticRegression())\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "report = classification_report(y_test, y_pred, target_names=y.columns)\n",
    "print(report)\n",
    "\n",
    "# Save the vectorizer and model\n",
    "vectorizer_path = 'tfidf_vectorizer.pkl'\n",
    "model_path = 'multi_label_model.pkl'\n",
    "joblib.dump(vectorizer, vectorizer_path)\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "print(\"Vectorizer and model saved to disk.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
